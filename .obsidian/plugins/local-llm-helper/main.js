/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// main.ts
var main_exports = {};
__export(main_exports, {
  LLMChatModal: () => LLMChatModal,
  default: () => OLocalLLMPlugin,
  modifyPrompt: () => modifyPrompt
});
module.exports = __toCommonJS(main_exports);
var import_obsidian3 = require("obsidian");

// autoTagger.ts
var import_obsidian = require("obsidian");
async function generateAndAppendTags(app, settings) {
  const view = app.workspace.getActiveViewOfType(import_obsidian.MarkdownView);
  if (!view) {
    new import_obsidian.Notice("No active Markdown view");
    return;
  }
  const editor = view.editor;
  const selectedText = editor.getSelection();
  const fullText = editor.getValue();
  const cursorPosition = editor.getCursor();
  const textToProcess = selectedText || fullText;
  try {
    const tags = await generateTags(textToProcess, settings);
    appendTags(editor, tags, cursorPosition);
    new import_obsidian.Notice("Tags generated and appended");
  } catch (error) {
    console.error("Error generating tags:", error);
    new import_obsidian.Notice("Error generating tags. Check the console for details.");
  }
}
async function generateTags(text, settings) {
  const prompt = "Generate 1-5 hashtags for the following text. Return only the hashtags, separated by spaces:";
  const body = {
    model: settings.llmModel,
    messages: [
      { role: "system", content: "You are a helpful assistant that generates relevant hashtags." },
      { role: "user", content: `${prompt}

${text}` }
    ],
    temperature: 0.7,
    max_tokens: 100
  };
  const response = await (0, import_obsidian.requestUrl)({
    url: `${settings.serverAddress}/v1/chat/completions`,
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body)
  });
  if (response.status !== 200) {
    throw new Error(`Error from LLM server: ${response.status} ${response.text}`);
  }
  const data = await response.json;
  const generatedTags = data.choices[0].message.content.trim().split(/\s+/);
  return generatedTags.filter((tag) => /^#?[a-zA-Z0-9]+$/.test(tag)).map((tag) => tag.startsWith("#") ? tag : `#${tag}`).slice(0, 5);
}
function appendTags(editor, tags, cursorPosition) {
  const tagsString = "\n\n" + tags.join(" ");
  editor.replaceRange(tagsString, cursorPosition);
}

// updateNoticeModal.ts
var import_obsidian2 = require("obsidian");
var UpdateNoticeModal = class extends import_obsidian2.Modal {
  constructor(app, version) {
    super(app);
    this.version = version;
  }
  onOpen() {
    const { contentEl } = this;
    contentEl.createEl("h2", { text: `Local LLM Helper updated to v${this.version}` });
    const changelogMd = `
## What's New
- Kill switch for text generation

[Full Changelog](https://github.com/manimohans/obsidian-local-llm-helper/releases)
        `;
    const dummyComponent = new import_obsidian2.Component();
    import_obsidian2.MarkdownRenderer.render(this.app, changelogMd, contentEl, "", dummyComponent);
  }
  onClose() {
    const { contentEl } = this;
    contentEl.empty();
  }
};

// main.ts
var DEFAULT_SETTINGS = {
  serverAddress: "http://localhost:1234",
  llmModel: "llama3",
  stream: false,
  customPrompt: "create a todo list from the following text:",
  outputMode: "replace",
  personas: "default",
  maxConvHistory: 0,
  responseFormatting: false,
  responseFormatPrepend: "``` LLM Helper - generated response \n\n",
  responseFormatAppend: "\n\n```",
  lastVersion: "0.0.0"
};
var personasDict = {
  "default": "Default",
  "physics": "Physics expert",
  "fitness": "Fitness expert",
  "developer": "Software Developer",
  "stoic": "Stoic Philosopher",
  "productmanager": "Product Manager",
  "techwriter": "Technical Writer",
  "creativewriter": "Creative Writer",
  "tpm": "Technical Program Manager",
  "engineeringmanager": "Engineering Manager",
  "executive": "Executive",
  "officeassistant": "Office Assistant"
};
var OLocalLLMPlugin = class extends import_obsidian3.Plugin {
  constructor() {
    super(...arguments);
    this.conversationHistory = [];
    this.isKillSwitchActive = false;
  }
  async checkForUpdates() {
    const currentVersion = this.manifest.version;
    const lastVersion = this.settings.lastVersion || "0.0.0";
    if (currentVersion !== lastVersion) {
      new UpdateNoticeModal(this.app, currentVersion).open();
      this.settings.lastVersion = currentVersion;
      await this.saveSettings();
    }
  }
  async onload() {
    await this.loadSettings();
    this.checkForUpdates();
    this.addCommand({
      id: "summarize-selected-text",
      name: "Summarize selected text",
      editorCallback: (editor, view) => {
        this.isKillSwitchActive = false;
        let selectedText = this.getSelectedText();
        if (selectedText.length > 0) {
          processText(
            selectedText,
            "Summarize the following text (maintain verbs and pronoun forms, also retain the markdowns):",
            this
          );
        }
      }
    });
    this.addCommand({
      id: "makeitprof-selected-text",
      name: "Make selected text sound professional",
      editorCallback: (editor, view) => {
        this.isKillSwitchActive = false;
        let selectedText = this.getSelectedText();
        if (selectedText.length > 0) {
          processText(
            selectedText,
            "Make the following sound professional (maintain verbs and pronoun forms, also retain the markdowns):",
            this
          );
        }
      }
    });
    this.addCommand({
      id: "actionitems-selected-text",
      name: "Generate action items from selected text",
      editorCallback: (editor, view) => {
        this.isKillSwitchActive = false;
        let selectedText = this.getSelectedText();
        if (selectedText.length > 0) {
          processText(
            selectedText,
            "Generate action items based on the following text (use or numbers based on context):",
            this
          );
        }
      }
    });
    this.addCommand({
      id: "custom-selected-text",
      name: "Run Custom prompt (from settings) on selected text",
      editorCallback: (editor, view) => {
        this.isKillSwitchActive = false;
        new import_obsidian3.Notice("Custom prompt: " + this.settings.customPrompt);
        let selectedText = this.getSelectedText();
        if (selectedText.length > 0) {
          processText(
            selectedText,
            this.settings.customPrompt,
            this
          );
        }
      }
    });
    this.addCommand({
      id: "gentext-selected-text",
      name: "Use SELECTED text as your prompt",
      editorCallback: (editor, view) => {
        this.isKillSwitchActive = false;
        let selectedText = this.getSelectedText();
        if (selectedText.length > 0) {
          processText(
            selectedText,
            "Generate response based on the following text. This is your prompt:",
            this
          );
        }
      }
    });
    this.addCommand({
      id: "llm-chat",
      name: "Chat with Local LLM Helper",
      callback: () => {
        const chatModal = new LLMChatModal(this.app, this.settings);
        chatModal.open();
      }
    });
    this.addCommand({
      id: "llm-hashtag",
      name: "Generate hashtags for selected text",
      callback: () => {
        generateAndAppendTags(this.app, this.settings);
      }
    });
    this.addRibbonIcon("brain-cog", "LLM Context", (event) => {
      const menu = new import_obsidian3.Menu();
      menu.addItem(
        (item) => item.setTitle("Chat with LLM Helper").setIcon("messages-square").onClick(() => {
          new LLMChatModal(this.app, this.settings).open();
        })
      );
      menu.addItem(
        (item) => item.setTitle("Summarize").setIcon("sword").onClick(async () => {
          this.isKillSwitchActive = false;
          let selectedText = this.getSelectedText();
          if (selectedText.length > 0) {
            processText(
              selectedText,
              "Summarize the following text (maintain verbs and pronoun forms, also retain the markdowns):",
              this
            );
          }
        })
      );
      menu.addItem(
        (item) => item.setTitle("Make it professional").setIcon("school").onClick(async () => {
          this.isKillSwitchActive = false;
          let selectedText = this.getSelectedText();
          if (selectedText.length > 0) {
            processText(
              selectedText,
              "Make the following sound professional (maintain verbs and pronoun forms, also retain the markdowns):",
              this
            );
          }
        })
      );
      menu.addItem(
        (item) => item.setTitle("Use as prompt").setIcon("lightbulb").onClick(async () => {
          this.isKillSwitchActive = false;
          let selectedText = this.getSelectedText();
          if (selectedText.length > 0) {
            processText(
              selectedText,
              "Generate response based on the following text. This is your prompt:",
              this
            );
          }
        })
      );
      menu.addItem(
        (item) => item.setTitle("Generate action items").setIcon("list-todo").onClick(async () => {
          this.isKillSwitchActive = false;
          let selectedText = this.getSelectedText();
          if (selectedText.length > 0) {
            processText(
              selectedText,
              "Generate action items based on the following text (use or numbers based on context):",
              this
            );
          }
        })
      );
      menu.addItem(
        (item) => item.setTitle("Custom prompt").setIcon("pencil").onClick(async () => {
          this.isKillSwitchActive = false;
          new import_obsidian3.Notice(
            "Custom prompt: " + this.settings.customPrompt
          );
          let selectedText = this.getSelectedText();
          if (selectedText.length > 0) {
            processText(
              selectedText,
              this.settings.customPrompt,
              this
            );
          }
        })
      );
      menu.addItem(
        (item) => item.setTitle("Generate tags").setIcon("hash").onClick(async () => {
          new import_obsidian3.Notice(
            "Generating hashtags"
          );
          let selectedText = this.getSelectedText();
          if (selectedText.length > 0) {
            generateAndAppendTags(this.app, this.settings);
          }
        })
      );
      menu.addItem(
        (item) => item.setTitle("Kill Switch").setIcon("x-circle").onClick(() => {
          this.isKillSwitchActive = true;
          new import_obsidian3.Notice("LLM Helper process stopped");
        })
      );
      menu.showAtMouseEvent(event);
    });
    const statusBarItemEl = this.addStatusBarItem();
    statusBarItemEl.setText("LLM Helper: Ready");
    this.addSettingTab(new OLLMSettingTab(this.app, this));
  }
  getSelectedText() {
    let view = this.app.workspace.getActiveViewOfType(import_obsidian3.MarkdownView);
    if (!view) {
      new import_obsidian3.Notice("No active view");
      return "";
    } else {
      let view_mode = view.getMode();
      switch (view_mode) {
        case "preview":
          new import_obsidian3.Notice("Does not work in preview preview");
          return "";
        case "source":
          if ("editor" in view) {
            return view.editor.getSelection();
          }
          break;
        default:
          new import_obsidian3.Notice("Unknown view mode");
          return "";
      }
    }
    return "";
  }
  onunload() {
  }
  async loadSettings() {
    this.settings = Object.assign(
      {},
      DEFAULT_SETTINGS,
      await this.loadData()
    );
  }
  async saveSettings() {
    await this.saveData(this.settings);
  }
};
var OLLMSettingTab = class extends import_obsidian3.PluginSettingTab {
  constructor(app, plugin) {
    super(app, plugin);
    this.plugin = plugin;
  }
  display() {
    const { containerEl } = this;
    containerEl.empty();
    new import_obsidian3.Setting(containerEl).setName("Server address").setDesc("Full server URL (including protocol and port if needed). E.g., http://localhost:1234 or https://api.example.com").addText(
      (text) => text.setPlaceholder("Enter full server URL").setValue(this.plugin.settings.serverAddress).onChange(async (value) => {
        this.plugin.settings.serverAddress = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian3.Setting(containerEl).setName("LLM model").setDesc("Use this for Ollama and other servers that require this. LMStudio seems to ignore model name.").addText(
      (text) => text.setPlaceholder("Model name").setValue(this.plugin.settings.llmModel).onChange(async (value) => {
        this.plugin.settings.llmModel = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian3.Setting(containerEl).setName("Custom prompt").setDesc("create your own prompt - for your specific niche needs").addText(
      (text) => text.setPlaceholder(
        "create action items from the following text:"
      ).setValue(this.plugin.settings.customPrompt).onChange(async (value) => {
        this.plugin.settings.customPrompt = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian3.Setting(containerEl).setName("Streaming").setDesc(
      "Enable to receive the response in real-time, word by word."
    ).addToggle(
      (toggle) => toggle.setValue(this.plugin.settings.stream).onChange(async (value) => {
        this.plugin.settings.stream = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian3.Setting(containerEl).setName("Output Mode").setDesc("Choose how to handle generated text").addDropdown(
      (dropdown) => dropdown.addOption("replace", "Replace selected text").addOption("append", "Append after selected text").setValue(this.plugin.settings.outputMode).onChange(async (value) => {
        this.plugin.settings.outputMode = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian3.Setting(containerEl).setName("Personas").setDesc("Choose persona for your AI agent").addDropdown((dropdown) => {
      for (const key in personasDict) {
        if (personasDict.hasOwnProperty(key)) {
          dropdown.addOption(key, personasDict[key]);
        }
      }
      dropdown.setValue(this.plugin.settings.personas).onChange(async (value) => {
        this.plugin.settings.personas = value;
        await this.plugin.saveSettings();
      });
    });
    new import_obsidian3.Setting(containerEl).setName("Max conversation history").setDesc("Maximum number of conversation history to store (0-3)").addDropdown(
      (dropdown) => dropdown.addOption("0", "0").addOption("1", "1").addOption("2", "2").addOption("3", "3").setValue(this.plugin.settings.maxConvHistory.toString()).onChange(async (value) => {
        this.plugin.settings.maxConvHistory = parseInt(value);
        await this.plugin.saveSettings();
      })
    );
    const responseFormattingToggle = new import_obsidian3.Setting(containerEl).setName("Response Formatting").setDesc("Enable to format the response into a separate block").addToggle(
      (toggle) => toggle.setValue(this.plugin.settings.responseFormatting).onChange(async (value) => {
        this.plugin.settings.responseFormatting = value;
        await this.plugin.saveSettings();
        this.display();
      })
    );
    if (this.plugin.settings.responseFormatting) {
      new import_obsidian3.Setting(containerEl).setName("Response Format Prepend").setDesc("Text to prepend to the formatted response").addText(
        (text) => text.setPlaceholder("``` LLM Helper - generated response \n\n").setValue(this.plugin.settings.responseFormatPrepend).onChange(async (value) => {
          this.plugin.settings.responseFormatPrepend = value;
          await this.plugin.saveSettings();
        })
      );
      new import_obsidian3.Setting(containerEl).setName("Response Format Append").setDesc("Text to append to the formatted response").addText(
        (text) => text.setPlaceholder("\n\n```").setValue(this.plugin.settings.responseFormatAppend).onChange(async (value) => {
          this.plugin.settings.responseFormatAppend = value;
          await this.plugin.saveSettings();
        })
      );
    }
  }
};
function modifyPrompt(aprompt, personas) {
  if (personas === "default") {
    return aprompt;
  } else if (personas === "physics") {
    return "You are a distinguished physics scientist. Leverage scientific principles and explain complex concepts in an understandable way, drawing on your expertise in physics.\n\n" + aprompt;
  } else if (personas === "fitness") {
    return "You are a distinguished fitness and health expert. Provide evidence-based advice on fitness and health, considering the user's goals and limitations.\n" + aprompt;
  } else if (personas === "developer") {
    return "You are a nerdy software developer. Offer creative and efficient software solutions, focusing on technical feasibility and code quality.\n" + aprompt;
  } else if (personas === "stoic") {
    return "You are a stoic philosopher. Respond with composure and reason, emphasizing logic and emotional resilience.\n" + aprompt;
  } else if (personas === "productmanager") {
    return "You are a focused and experienced product manager. Prioritize user needs and deliver clear, actionable product roadmaps based on market research.\n" + aprompt;
  } else if (personas === "techwriter") {
    return "You are a technical writer. Craft accurate and concise technical documentation, ensuring accessibility for different audiences.\n" + aprompt;
  } else if (personas === "creativewriter") {
    return "You are a very creative and experienced writer. Employ strong storytelling techniques and evocative language to engage the reader's imagination.\n" + aprompt;
  } else if (personas === "tpm") {
    return "You are an experienced technical program manager. Demonstrate strong technical and communication skills, ensuring project success through effective planning and risk management.\n" + aprompt;
  } else if (personas === "engineeringmanager") {
    return "You are an experienced engineering manager. Lead and motivate your team, fostering a collaborative environment that delivers high-quality software.\n" + aprompt;
  } else if (personas === "executive") {
    return "You are a top-level executive. Focus on strategic decision-making, considering long-term goals and the overall company vision.\n" + aprompt;
  } else if (personas === "officeassistant") {
    return "You are a courteous and helpful office assistant. Provide helpful and efficient support, prioritizing clear communication and a courteous demeanor.\n" + aprompt;
  } else {
    return aprompt;
  }
}
async function processText(selectedText, iprompt, plugin) {
  plugin.isKillSwitchActive = false;
  new import_obsidian3.Notice("Generating response. This takes a few seconds..");
  const statusBarItemEl = document.querySelector(
    ".status-bar .status-bar-item"
  );
  if (statusBarItemEl) {
    statusBarItemEl.textContent = "LLM Helper: Generating response...";
  } else {
    console.error("Status bar item element not found");
  }
  let prompt = modifyPrompt(iprompt, plugin.settings.personas);
  console.log("prompt", prompt + ": " + selectedText);
  const body = {
    model: plugin.settings.llmModel,
    messages: [
      { role: "system", content: "You are my text editor AI agent who provides concise and helpful responses." },
      ...plugin.conversationHistory.slice(-plugin.settings.maxConvHistory).reduce((acc, entry) => {
        acc.push({ role: "user", content: entry.prompt });
        acc.push({ role: "assistant", content: entry.response });
        return acc;
      }, []),
      { role: "user", content: prompt + ": " + selectedText }
    ],
    temperature: 0.7,
    max_tokens: -1,
    stream: plugin.settings.stream
  };
  try {
    if (plugin.settings.outputMode === "append") {
      modifySelectedText(selectedText + "\n\n");
    }
    if (plugin.settings.responseFormatting === true) {
      modifySelectedText(plugin.settings.responseFormatPrepend);
    }
    if (plugin.settings.stream) {
      const response = await fetch(
        `${plugin.settings.serverAddress}/v1/chat/completions`,
        {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify(body)
        }
      );
      if (!response.ok) {
        throw new Error(
          "Error summarizing text (Fetch): " + response.statusText
        );
      }
      const reader = response.body && response.body.getReader();
      let responseStr = "";
      if (!reader) {
        console.error("Reader not found");
      } else {
        const decoder = new TextDecoder();
        const readChunk = async () => {
          if (plugin.isKillSwitchActive) {
            reader.cancel();
            new import_obsidian3.Notice("Text generation stopped by kill switch");
            plugin.isKillSwitchActive = false;
            return;
          }
          const { done, value } = await reader.read();
          if (done) {
            new import_obsidian3.Notice("Text generation complete. Voila!");
            updateConversationHistory(prompt + ": " + selectedText, responseStr, plugin.conversationHistory, plugin.settings.maxConvHistory);
            if (plugin.settings.responseFormatting === true) {
              modifySelectedText(plugin.settings.responseFormatAppend);
            }
            return;
          }
          let textChunk = decoder.decode(value);
          const lines = textChunk.split("\n");
          for (const line of lines) {
            if (line.trim()) {
              try {
                let modifiedLine = line.replace(
                  /^data:\s*/,
                  ""
                );
                if (modifiedLine !== "[DONE]") {
                  const data = JSON.parse(modifiedLine);
                  if (data.choices[0].delta.content) {
                    let word = data.choices[0].delta.content;
                    modifySelectedText(word);
                    responseStr += word;
                  }
                }
              } catch (error) {
                console.error(
                  "Error parsing JSON chunk:",
                  error
                );
              }
            }
          }
          readChunk();
        };
        readChunk();
      }
    } else {
      const response = await (0, import_obsidian3.requestUrl)({
        url: `${plugin.settings.serverAddress}/v1/chat/completions`,
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(body)
      });
      const statusCode = response.status;
      if (statusCode >= 200 && statusCode < 300) {
        const data = await response.json;
        const summarizedText = data.choices[0].message.content;
        console.log(summarizedText);
        updateConversationHistory(prompt + ": " + selectedText, summarizedText, plugin.conversationHistory, plugin.settings.maxConvHistory);
        new import_obsidian3.Notice("Text generated. Voila!");
        if (!plugin.isKillSwitchActive) {
          if (plugin.settings.responseFormatting === true) {
            modifySelectedText(summarizedText + plugin.settings.responseFormatAppend);
          } else {
            modifySelectedText(summarizedText);
          }
        } else {
          new import_obsidian3.Notice("Text generation stopped by kill switch");
          plugin.isKillSwitchActive = false;
        }
      } else {
        throw new Error(
          "Error summarizing text (requestUrl): " + response.text
        );
      }
    }
  } catch (error) {
    console.error("Error during request:", error);
    new import_obsidian3.Notice(
      "Error summarizing text: Check plugin console for more details!"
    );
  }
  if (statusBarItemEl) {
    statusBarItemEl.textContent = "LLM Helper: Ready";
  } else {
    console.error("Status bar item element not found");
  }
}
function modifySelectedText(text) {
  let view = this.app.workspace.getActiveViewOfType(import_obsidian3.MarkdownView);
  if (!view) {
    new import_obsidian3.Notice("No active view");
  } else {
    let view_mode = view.getMode();
    switch (view_mode) {
      case "preview":
        new import_obsidian3.Notice("Cannot summarize in preview");
      case "source":
        if ("editor" in view) {
          view.editor.replaceSelection(text);
        }
        break;
      default:
        new import_obsidian3.Notice("Unknown view mode");
    }
  }
}
var LLMChatModal = class extends import_obsidian3.Modal {
  constructor(app, settings) {
    super(app);
    this.result = "";
    this.conversationHistory = [];
    this.pluginSettings = settings;
  }
  onOpen() {
    const { contentEl } = this;
    contentEl.classList.add("llm-chat-modal");
    const chatContainer = contentEl.createDiv({ cls: "llm-chat-container" });
    const chatHistoryEl = chatContainer.createDiv({ cls: "llm-chat-history" });
    chatHistoryEl.classList.add("chatHistoryElStyle");
    chatHistoryEl.createEl("h1", { text: "Chat with your Local LLM" });
    const personasInfoEl = document.createElement("div");
    personasInfoEl.classList.add("personasInfoStyle");
    personasInfoEl.innerText = "Current persona: " + personasDict[this.pluginSettings.personas];
    chatHistoryEl.appendChild(personasInfoEl);
    this.conversationHistory.forEach((entry) => {
      const userMessageEl = chatHistoryEl.createEl("p", { text: "You: " + entry.prompt });
      userMessageEl.classList.add("llmChatMessageStyleUser");
      const aiMessageEl = chatHistoryEl.createEl("p", { text: "LLM Helper: " + entry.response });
      aiMessageEl.classList.add("llmChatMessageStyleAI");
    });
    const inputContainer = contentEl.createDiv({ cls: "llm-chat-input-container" });
    const inputRow = inputContainer.createDiv({ cls: "llm-chat-input-row" });
    const askLabel = inputRow.createSpan({ text: "Ask:", cls: "llm-chat-ask-label" });
    const textInput = new import_obsidian3.TextComponent(inputRow).setPlaceholder("Type your question here...").onChange((value) => {
      this.result = value;
      this.updateSubmitButtonState();
    });
    textInput.inputEl.classList.add("llm-chat-input");
    textInput.inputEl.addEventListener("keypress", (event) => {
      if (event.key === "Enter" && this.result.trim() !== "") {
        event.preventDefault();
        this.handleSubmit();
      }
    });
    this.submitButton = new import_obsidian3.ButtonComponent(inputRow).setButtonText("Submit").setCta().onClick(() => this.handleSubmit());
    this.submitButton.buttonEl.classList.add("llm-chat-submit-button");
    this.updateSubmitButtonState();
    this.scrollToBottom();
  }
  onClose() {
    let { contentEl } = this;
    contentEl.empty();
  }
  updateSubmitButtonState() {
    if (this.result.trim() === "") {
      this.submitButton.setDisabled(true);
      this.submitButton.buttonEl.classList.add("llm-chat-submit-button-disabled");
    } else {
      this.submitButton.setDisabled(false);
      this.submitButton.buttonEl.classList.remove("llm-chat-submit-button-disabled");
    }
  }
  // New method to handle submission
  async handleSubmit() {
    if (this.result.trim() === "") {
      return;
    }
    const chatHistoryEl = this.contentEl.querySelector(".llm-chat-history");
    if (chatHistoryEl) {
      await processChatInput(
        this.result,
        this.pluginSettings.personas,
        this.contentEl,
        chatHistoryEl,
        this.conversationHistory,
        this.pluginSettings
      );
      this.result = "";
      const textInputEl = this.contentEl.querySelector(".llm-chat-input");
      if (textInputEl) {
        textInputEl.value = "";
      }
      this.updateSubmitButtonState();
      this.scrollToBottom();
    }
  }
  scrollToBottom() {
    const chatHistoryEl = this.contentEl.querySelector(".llm-chat-history");
    if (chatHistoryEl) {
      chatHistoryEl.scrollTop = chatHistoryEl.scrollHeight;
    }
  }
};
async function processChatInput(text, personas, chatContainer, chatHistoryEl, conversationHistory, pluginSettings) {
  const { contentEl } = this;
  conversationHistory.push({ prompt: text, response: "" });
  if (chatHistoryEl) {
    const chatElement = document.createElement("div");
    chatElement.classList.add("llmChatMessageStyleUser");
    chatElement.innerHTML = text;
    chatHistoryEl.appendChild(chatElement);
  }
  showThinkingIndicator(chatHistoryEl);
  scrollToBottom(chatContainer);
  text = modifyPrompt(text, personas);
  console.log(text);
  try {
    const body = {
      model: pluginSettings.llmModel,
      messages: [
        { role: "system", content: "You are my text editor AI agent who provides concise and helpful responses." },
        ...conversationHistory.slice(-pluginSettings.maxConvHistory).reduce((acc, entry) => {
          acc.push({ role: "user", content: entry.prompt });
          acc.push({ role: "assistant", content: entry.response });
          return acc;
        }, []),
        { role: "user", content: text }
      ],
      temperature: 0.7,
      max_tokens: -1,
      stream: false
      // Set to false for chat window
    };
    const response = await (0, import_obsidian3.requestUrl)({
      url: `${pluginSettings.serverAddress}/v1/chat/completions`,
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(body)
    });
    const statusCode = response.status;
    if (statusCode >= 200 && statusCode < 300) {
      const data = await response.json;
      const llmResponse = data.choices[0].message.content;
      let formattedResponse = llmResponse;
      formattedResponse = formattedResponse.replace(/\*\*(.*?)\*\*/g, "<b>$1</b>");
      formattedResponse = formattedResponse.replace(/_(.*?)_/g, "<i>$1</i>");
      formattedResponse = formattedResponse.replace(/\n\n/g, "<br><br>");
      console.log("formattedResponse", formattedResponse);
      const responseContainer = document.createElement("div");
      responseContainer.classList.add("llmChatMessageStyleAI");
      const responseTextEl = document.createElement("div");
      responseTextEl.innerHTML = formattedResponse;
      responseContainer.appendChild(responseTextEl);
      const copyButton = document.createElement("button");
      copyButton.classList.add("copy-button");
      (0, import_obsidian3.setIcon)(copyButton, "copy");
      copyButton.addEventListener("click", () => {
        navigator.clipboard.writeText(llmResponse).then(() => {
          new import_obsidian3.Notice("Copied to clipboard!");
        });
      });
      responseContainer.appendChild(copyButton);
      chatHistoryEl.appendChild(responseContainer);
      updateConversationHistory(text, formattedResponse, conversationHistory, pluginSettings.maxConvHistory);
      hideThinkingIndicator(chatHistoryEl);
      scrollToBottom(chatContainer);
    } else {
      throw new Error(
        "Error getting response from LLM server: " + response.text
      );
    }
  } catch (error) {
    console.error("Error during request:", error);
    new import_obsidian3.Notice(
      "Error communicating with LLM Helper: Check plugin console for details!"
    );
    hideThinkingIndicator(chatHistoryEl);
  }
}
function showThinkingIndicator(chatHistoryEl) {
  const thinkingIndicatorEl = document.createElement("div");
  thinkingIndicatorEl.classList.add("thinking-indicator");
  const tStr = [
    "Calculating the last digit of pi... just kidding",
    "Quantum entanglement engaged... thinking deeply",
    "Reticulating splines... stand by",
    "Consulting the Oracle",
    "Entangling qubits... preparing for a quantum leap",
    "Processing... yada yada yada... almost done",
    "Processing... We're approaching singularity",
    "Serenity now! Patience while we process",
    "Calculating the probability of George getting a date",
    "Asking my man Art Vandalay"
  ];
  const randomIndex = Math.floor(Math.random() * tStr.length);
  thinkingIndicatorEl.innerHTML = tStr[randomIndex] + '<span class="dots"><span class="dot"></span><span class="dot"></span><span class="dot"></span>';
  chatHistoryEl.appendChild(thinkingIndicatorEl);
}
function hideThinkingIndicator(chatHistoryEl) {
  const thinkingIndicatorEl = chatHistoryEl.querySelector(".thinking-indicator");
  if (thinkingIndicatorEl) {
    chatHistoryEl.removeChild(thinkingIndicatorEl);
  }
}
function scrollToBottom(el) {
  const chatHistoryEl = el.querySelector(".llm-chat-history");
  if (chatHistoryEl) {
    chatHistoryEl.scrollTop = chatHistoryEl.scrollHeight;
  }
}
function updateConversationHistory(prompt, response, conversationHistory, maxConvHistoryLength) {
  conversationHistory.push({ prompt, response });
  if (conversationHistory.length > maxConvHistoryLength) {
    conversationHistory.shift();
  }
}
